{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession  \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MovieLens Recommendation System\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_users_data = \"./data/ml-1m/users.dat\"\n",
    "path_to_movies_data = \"./data/ml-1m/movies.dat\"\n",
    "path_to_ratings_data = \"./data/ml-1m/ratings.dat\"\n",
    "\n",
    "users_column_names = [\"user_id\", \"gender\", \"age\", \"occupation\", \"zip_code\"]\n",
    "movies_column_names = [\"movie_id\", \"title\", \"genres\"]\n",
    "ratings_column_names = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
    "\n",
    "user_df = spark.read.csv(path_to_users_data,\n",
    "                          sep='::',\n",
    "                          header=False,\n",
    "                          inferSchema=True)\n",
    "movie_df = spark.read.csv(path_to_movies_data,\n",
    "                           sep='::',\n",
    "                           header=False,\n",
    "                           inferSchema=True)\n",
    "rating_df = spark.read.csv(path_to_ratings_data,\n",
    "                            sep='::',\n",
    "                            header=False,\n",
    "                            inferSchema=True)\n",
    "user_df = user_df.toDF(*users_column_names)\n",
    "movie_df = movie_df.toDF(*movies_column_names)\n",
    "rating_df = rating_df.toDF(*ratings_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数来计算IQR并去除异常值\n",
    "def remove_outliers(df, column):\n",
    "    # 计算四分位数\n",
    "    quantiles = df.approxQuantile(column, [0.25, 0.75], 0.05)\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # 计算异常值的边界\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # 过滤掉异常值\n",
    "    return df.filter((F.col(column) >= lower_bound) & (F.col(column) <= upper_bound))\n",
    "\n",
    "rating_df = remove_outliers(rating_df, 'rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = rating_df.join(movie_df, 'movie_id', 'left')\n",
    "movie_df = rating_df.select(['movie_id', 'title', 'genres']).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个 UDF (User Defined Function) 来进行评分的分段\n",
    "def rating_segment(avg_rating):\n",
    "    if 1 <= avg_rating < 1.5:\n",
    "        return 1\n",
    "    elif 1.5 <= avg_rating < 2:\n",
    "        return 2\n",
    "    elif 2 <= avg_rating < 2.5:\n",
    "        return 3\n",
    "    elif 2.5 <= avg_rating < 3:\n",
    "        return 4\n",
    "    elif 3 <= avg_rating < 3.5:\n",
    "        return 5\n",
    "    elif 3.5 <= avg_rating < 4:\n",
    "        return 6\n",
    "    elif 4 <= avg_rating < 4.5:\n",
    "        return 7\n",
    "    elif 4.5 <= avg_rating <= 5:\n",
    "        return 8\n",
    "    else:\n",
    "        return None  # 处理无效的评分\n",
    "\n",
    "# 将 UDF 注册到 Spark Session\n",
    "segmented_rating_udf = F.udf(rating_segment)\n",
    "\n",
    "window_user = Window.partitionBy(\"user_id\")\n",
    "rating_df = rating_df.withColumn('user_avg_rating', F.avg('rating').over(window_user))\n",
    "rating_df = rating_df.withColumn('label',\n",
    "                                 F.when((F.col('rating') >= F.col('user_avg_rating')), 1).otherwise(0))\n",
    "rating_df = rating_df.withColumn('user_avg_rating', segmented_rating_udf(F.col('user_avg_rating')))\n",
    "\n",
    "window_movie = Window.partitionBy(\"movie_id\")\n",
    "rating_df = rating_df.withColumn('movie_avg_rating', F.avg('rating').over(window_movie))\n",
    "rating_df = rating_df.withColumn('movie_avg_rating', segmented_rating_udf(F.col('movie_avg_rating')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.filter(F.col('user_avg_rating').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = rating_df.select(['movie_id', 'title', 'genres', 'movie_avg_rating']).distinct()\n",
    "# 定义一个窗口规范，这里我们不使用分区，所以整个DataFrame被视为一个单一的分区  \n",
    "windowSpec = Window.orderBy(F.col(\"movie_id\").asc())  \n",
    "# 使用row_number()生成一个新的ID列，命名为new_movie_id  \n",
    "movie_df = movie_df.withColumn(\"new_movie_id\", F.row_number().over(windowSpec))  \n",
    "movie_df = movie_df.drop('movie_id')\n",
    "movie_df = movie_df.withColumnRenamed('new_movie_id', 'movie_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_df = rating_df.select(['user_id', 'user_avg_rating']).distinct()\n",
    "user_df = user_df.join(u_df, ['user_id'], 'left').select(['user_id', 'gender', 'age', 'occupation', 'user_avg_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = rating_df.drop(*['movie_id', 'genres', 'movie_avg_rating'])\n",
    "rating_df = rating_df.join(movie_df, 'title')\n",
    "rating_df = rating_df.drop(*['title'])\n",
    "movie_df = movie_df.drop(*['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_num = movie_df.filter(~F.col('movie_avg_rating').isNotNull()).count()\n",
    "null_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.write.mode('overwrite').parquet('./user_info_dcn.parquet')\n",
    "movie_df.write.mode('overwrite').parquet('./movie_info_dcn.parquet')\n",
    "rating_df.write.mode('overwrite').parquet('./rating_info_dcn.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重复值检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_duplicates(df, cols=None):\n",
    "    \"\"\"  \n",
    "    检测DataFrame中是否存在重复的行。  \n",
    "  \n",
    "    :param df: pyspark.sql.DataFrame  \n",
    "        要检查的DataFrame。  \n",
    "    :param cols: list of str, optional  \n",
    "        用于检测重复项的列名列表。如果为None，则使用所有列。  \n",
    "    :return: bool  \n",
    "        如果存在重复行，则返回True；否则返回False。  \n",
    "    \"\"\"\n",
    "    # 如果cols为None，则使用所有列\n",
    "    if cols is None:\n",
    "        cols = df.columns\n",
    "\n",
    "    # 去除指定列上的重复行\n",
    "    df_no_duplicates = df.dropDuplicates(cols)\n",
    "\n",
    "    # 计算原始DataFrame和去除重复项后的DataFrame的行数\n",
    "    original_count = df.count()\n",
    "    no_duplicates_count = df_no_duplicates.count()\n",
    "\n",
    "    # 如果行数不同，则表示存在重复项\n",
    "    return original_count != no_duplicates_count\n",
    "\n",
    "\n",
    "# 调用函数并打印结果\n",
    "print(has_duplicates(df))  \n",
    "# 检测特定列是否存在重复\n",
    "# print(has_duplicates(data_df, [\"user_id\", \"movie_id\", \"timestamp\"])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "缺失值检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_values(df):\n",
    "    \"\"\"\n",
    "    This function takes a DataFrame df as input and prints the names of columns with missing values.\n",
    "    It selects the first column of the DataFrame for counting missing values.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame to check for missing values.\n",
    "    \"\"\"\n",
    "    first_column = df.columns[0]  # Get the name of the first column\n",
    "\n",
    "    for column_name in df.columns:\n",
    "        # Filter rows where the current column is null, select the first column and count them\n",
    "        missing_value = df.filter(\n",
    "            F.col(column_name).isNull()).select(first_column)\n",
    "        count_missing = missing_value.count()\n",
    "\n",
    "        # If there are missing values, print the column name\n",
    "        if count_missing > 0:\n",
    "            print(f\"Column '{column_name}' has missing values.\")\n",
    "        else:\n",
    "            print(f\"Column '{column_name}' does not have missing values.\")\n",
    "    return print(\"All columns checked.\")\n",
    "\n",
    "\n",
    "find_missing_values(user_df)\n",
    "find_missing_values(movie_df)\n",
    "find_missing_values(rating_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession  \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window  \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import random\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MovieLens Recommendation System\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = spark.read.parquet('./user_info.parquet')\n",
    "movie_df = spark.read.parquet('./movie_info.parquet')\n",
    "rating_df = spark.read.parquet('./rating_info.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_df.count(), movie_df.count(), rating_df.count())\n",
    "print(rating_df.select('user_id').distinct().count(),\\\n",
    "      rating_df.select('movie_id').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 `df` 是你的 DataFrame，`column_name` 是你要检查的列名\n",
    "null_count = movie_df.filter(F.col('movie_avg_rating').isNull()).count()\n",
    "null_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHotEncoder(df, col_name):\n",
    "    unique_col = df.select(col_name).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    unique_col.sort()\n",
    "    for item in unique_col:\n",
    "        df = df.withColumn(str(item), F.when(F.col(col_name)==item, 1).otherwise(0))\n",
    "    # 使用VectorAssembler将多个二进制列合并成一个向量列\n",
    "    assembler = VectorAssembler(inputCols=[str(item) for item in unique_col],\n",
    "                                outputCol=col_name+'Vector')\n",
    "    df = assembler.transform(df)\n",
    "    # 删除中间创建的one-hot编码列\n",
    "    for item in unique_col:\n",
    "        df = df.drop(str(item))\n",
    "    df = df.drop(col_name)\n",
    "    return df\n",
    "\n",
    "def MultiHotEncoder(df, col_name):\n",
    "    # 对genres进行多热编码\n",
    "    # 拆分genres列\n",
    "    col_split = df.withColumn(col_name, F.explode(F.split(F.col(col_name), \"\\\\|\")))\n",
    "    # 获取所有genre的列表\n",
    "    unique_col = col_split.select(col_name).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    # 对每个genre进行multi-hot编码\n",
    "    for item in unique_col:\n",
    "        if isinstance(item, str):\n",
    "            df = df.withColumn(item, F.when(F.col(col_name).contains(item), 1).otherwise(0))\n",
    "    # 使用VectorAssembler将多个二进制列合并成一个向量列\n",
    "    assembler = VectorAssembler(inputCols=[item for item in unique_col if isinstance(item, str)],\n",
    "                                outputCol=col_name+'Vector')\n",
    "    df = assembler.transform(df)\n",
    "    # 删除中间创建的one-hot编码列\n",
    "    for item in unique_col:\n",
    "        if isinstance(item, str):\n",
    "            df = df.drop(item)\n",
    "    df = df.drop(col_name)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def user_data_processing(df):\n",
    "    # 将gender改为二值变量\n",
    "    df = df.withColumn(\"gender\",\n",
    "                                F.when(df[\"gender\"] == \"F\", 0).otherwise(1))\n",
    "    # 对occupation进行独热编码\n",
    "    df = OneHotEncoder(df, 'occupation')\n",
    "    # 对age进行独热编码\n",
    "    df = OneHotEncoder(df, 'age')\n",
    "    # 对user_avg_rating进行独热编码\n",
    "    df = OneHotEncoder(df, 'user_avg_rating')\n",
    "    return df\n",
    "\n",
    "def movie_data_processing(df):\n",
    "    # 对genres进行独热编码\n",
    "    df = MultiHotEncoder(df, 'genres')\n",
    "    # 对movie_avg_rating进行独热编码\n",
    "    df = OneHotEncoder(df, 'movie_avg_rating')\n",
    "    # 同时删除title和genres列\n",
    "    return df\n",
    "\n",
    "# 假设data_df是合并后的DataFrame\n",
    "user_df = user_data_processing(user_df)\n",
    "movie_df = movie_data_processing(movie_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.write.mode('overwrite').parquet('./user_df.parquet')\n",
    "movie_df.write.mode('overwrite').parquet('./movie_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = rating_df.drop(*['user_avg_rating', 'movie_avg_rating', 'genres'])\n",
    "rating_df = rating_df.join(user_df, 'user_id')\n",
    "rating_df = rating_df.join(movie_df, 'movie_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.write.mode('overwrite').parquet('./rating_df_dcn.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_num = rating_df.filter(F.col('movie_avg_ratingVector').isNull()).count()\n",
    "null_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "age与age_vector的对应关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapage = rating_df.select(['user_id', 'ageVector']).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_age2vector = rating_df.select(['age', 'ageVector']).distinct().show()\n",
    "map_occupation2vector = rating_df.select(['occupation', 'occupationVector']).distinct().show()\n",
    "map_user_avg_rating2vector = rating_df.select(['user_avg_rating', 'user_avg_ratingVector']).distinct().show()\n",
    "map_movie_avg_rating2vector = rating_df.select(['movie_avg_rating', 'movie_avg_ratingVector']).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模拟电影冷启动数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def select_five_movies(data_df, num_movies=5):\n",
    "#     # 获取唯一的movie_id列表\n",
    "#     unique_movie_ids = data_df.select(\"movie_id\").distinct().collect()\n",
    "#     # 将DataFrame的行转换为Python列表\n",
    "#     unique_movie_ids_list = [row[\"movie_id\"] for row in unique_movie_ids]\n",
    "#     # 随机选择num_movies个不同的电影ID\n",
    "#     sampled_movie_ids = random.sample(unique_movie_ids_list, num_movies)\n",
    "\n",
    "#     # 对每个选中的电影ID进行分组并计数\n",
    "#     movie_counts = data_df.groupBy(\"movie_id\").count()\n",
    "#     # 过滤出选中的电影ID的计数\n",
    "#     sampled_movie_counts = movie_counts.filter(\n",
    "#         F.col(\"movie_id\").isin(sampled_movie_ids)).collect()\n",
    "\n",
    "#     # 打印每个选中的电影的评分记录总数\n",
    "#     for row in sampled_movie_counts:\n",
    "#         print(f\"Movie ID {row['movie_id']} has {row['count']} ratings.\")\n",
    "\n",
    "#     # 返回选定的movie_ids\n",
    "#     return sampled_movie_ids\n",
    "\n",
    "\n",
    "# # 调用函数并打印结果\n",
    "# sampled_movie_ids = select_five_movies(rating_df)\n",
    "# print(\"Selected movie IDs:\", sampled_movie_ids)\n",
    "\n",
    "# # 根据随机选择的movie_id创建备份DataFrame\n",
    "# cold_movies_df = rating_df.filter(F.col(\"movie_id\").isin(sampled_movie_ids))\n",
    "\n",
    "# # 可以选择将备份DataFrame写入磁盘或进行其他处理\n",
    "# cold_movies_df.write.mode('overwrite').parquet(\n",
    "#     './movies_cold_data.parquet')\n",
    "\n",
    "# # 现在你可以使用sampled_movie_ids来从data_df中移除对应的评分记录\n",
    "# filtered_data_df = rating_df.filter(~F.col(\"movie_id\").isin(sampled_movie_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模拟用户冷启动数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def select_five_users(data_df, num_users=5):\n",
    "#     # 获取唯一的movie_id列表\n",
    "#     unique_user_ids = data_df.select(\"user_id\").distinct().collect()\n",
    "#     # 将DataFrame的行转换为Python列表\n",
    "#     unique_user_ids_list = [row[\"user_id\"] for row in unique_user_ids]\n",
    "#     # 随机选择num_movies个不同的电影ID\n",
    "#     sampled_user_ids = random.sample(unique_user_ids_list, num_users)\n",
    "\n",
    "#     # 对每个选中的电影ID进行分组并计数\n",
    "#     user_counts = data_df.groupBy(\"user_id\").count()\n",
    "#     # 过滤出选中的电影ID的计数\n",
    "#     sampled_user_counts = user_counts.filter(\n",
    "#         F.col(\"user_id\").isin(sampled_user_ids)).collect()\n",
    "\n",
    "#     # 打印每个选中的电影的评分记录总数\n",
    "#     for row in sampled_user_counts:\n",
    "#         print(f\"User ID {row['user_id']} has {row['count']} ratings.\")\n",
    "\n",
    "#     # 返回选定的movie_ids\n",
    "#     return sampled_user_ids\n",
    "\n",
    "\n",
    "# # 调用函数并打印结果\n",
    "# sampled_user_ids = select_five_users(filtered_data_df)\n",
    "# print(\"Selected user IDs:\", sampled_user_ids)\n",
    "\n",
    "# # 根据随机选择的movie_id创建备份DataFrame\n",
    "# cold_users_df = filtered_data_df.filter(F.col(\"user_id\").isin(sampled_user_ids))\n",
    "\n",
    "# # 可以选择将备份DataFrame写入磁盘或进行其他处理\n",
    "# cold_users_df.write.mode('overwrite').parquet(\n",
    "#     './users_cold_data.parquet')\n",
    "\n",
    "# # 现在你可以使用sampled_movie_ids来从data_df中移除对应的评分记录\n",
    "# filtered_data_df = filtered_data_df.filter(~F.col(\"user_id\").isin(sampled_user_ids))\n",
    "# # 现在你可以使用sampled_user_ids来从ratings_df中移除评分记录\n",
    "# filtered_data_df = filtered_data_df.filter(\n",
    "#     ~filtered_data_df[\"user_id\"].isin(sampled_user_ids))\n",
    "# filtered_data_df.write.mode('overwrite').parquet('./filtered_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成负样本用于模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"NegativeSampling\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = spark.read.parquet(\"./user_df.parquet\")\n",
    "movie_df = spark.read.parquet(\"./movie_df.parquet\")\n",
    "# 加载数据集\n",
    "rating_df = spark.read.parquet(\"./rating_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_df.count(), movie_df.count(), rating_df.count())\n",
    "print(rating_df.select('user_id').distinct().count(),\\\n",
    "      rating_df.select('movie_id').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成负样本的函数\n",
    "def generate_negative_samples(rating_df, df, data_user_positive_movies, sampling_ratio=2):\n",
    "\n",
    "    user_df = rating_df.select(\"user_id\", \"gender\", \"ageVector\", \"occupationVector\", 'user_avg_ratingVector').distinct()\n",
    "    movie_df = rating_df.select(\"movie_id\", \"genresVector\", 'movie_avg_ratingVector').distinct()\n",
    "    # 获取用户和电影的所有组合\n",
    "    user_all_movies = rating_df.select(\"user_id\", \"movie_id\").distinct()\n",
    "    all_movies = rating_df.select(\"movie_id\").distinct()\n",
    "    user_all_movies = user_all_movies.select(\"user_id\").distinct().crossJoin(all_movies)\n",
    "    user_positive_movies = df.select(\"user_id\", \"movie_id\").distinct()\n",
    "\n",
    "    # Step 3: 计算每个电影的点击频率\n",
    "    movie_click_freq = rating_df.groupBy(\"movie_id\").count().withColumnRenamed(\"count\", \"click_count\").select('movie_id', 'click_count')\n",
    "    # Step 4: 计算每个电影的权重（点击频率的0.75次幂）\n",
    "    movie_click_freq = movie_click_freq.withColumn(\"sampling_weight\", F.pow(F.col(\"click_count\"), 0.75))\n",
    "\n",
    "    # Step 5: 计算每个用户的正样本数量\n",
    "    user_positive_count = df.groupBy(\"user_id\").count().withColumnRenamed(\"count\", \"positive_count\")\n",
    "\n",
    "    # Step 6: 对每个用户生成负样本\n",
    "    user_negative_movies = user_all_movies.join(data_user_positive_movies, on=[\"user_id\", \"movie_id\"], how=\"left_anti\")\n",
    "    # Step 7: 合并负样本和电影点击频率，进行带权重的负采样\n",
    "    user_negative_movies = user_negative_movies.join(movie_click_freq, \"movie_id\", \"left\")\n",
    "\n",
    "    # 窗口是按user_id进行分区，随机排序后，按weight逆序排列\n",
    "    window_spec = Window.partitionBy(\"user_id\").orderBy(F.rand(42), F.col(\"sampling_weight\").desc())\n",
    "    user_negative_movies = user_negative_movies.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "    \n",
    "    # 按指定比例（默认2:1）采样负样本\n",
    "    user_negative_movies = user_negative_movies.join(user_positive_count, \"user_id\", 'left')\n",
    "    user_negative_movies = user_negative_movies.filter(F.col(\"row_num\") <= sampling_ratio * F.col(\"positive_count\"))  \n",
    "\n",
    "    # 正样本标签为1\n",
    "    user_positive_movies = user_positive_movies.withColumn(\"label\", F.lit(1))\n",
    "    # 负样本标签为-1\n",
    "    user_negative_movies = user_negative_movies.withColumn(\"label\", F.lit(0))  \n",
    "    user_negative_movies = user_negative_movies.select(\"user_id\", \"movie_id\", \"label\")\n",
    "\n",
    "    user_movie = user_positive_movies.union(user_negative_movies)\n",
    "    user_movie = user_movie.join(user_df, \"user_id\", \"left\")\n",
    "    user_movie = user_movie.join(movie_df, \"movie_id\", \"left\")\n",
    "    return user_movie\n",
    "\n",
    "# 创建窗口规范，按user_id分区，按timestamp降序排序\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(F.desc(\"timestamp\"))\n",
    "# 添加行号\n",
    "data_spark_df = rating_df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "\n",
    "\n",
    "# 创建训练、验证和测试集\n",
    "train_df = data_spark_df.filter(F.col(\"row_num\") > 10)\n",
    "val_df = data_spark_df.filter((F.col(\"row_num\") > 5) & (F.col(\"row_num\") <= 10))\n",
    "test_df = data_spark_df.filter(F.col(\"row_num\") <= 5)\n",
    "\n",
    "\n",
    "# 提取训练、验证和测试集中的所有正样本的电影ID\n",
    "train_positive_movies = train_df.select('user_id', \"movie_id\").distinct()\n",
    "val_positive_movies = val_df.select('user_id', \"movie_id\").distinct()\n",
    "test_positive_movies = test_df.select('user_id', \"movie_id\").distinct()\n",
    "val_positive_movies = train_positive_movies.union(val_positive_movies)\n",
    "test_positive_movies = val_positive_movies.union(test_positive_movies)\n",
    "\n",
    "# # 为训练集生成负样本\n",
    "train_samples = generate_negative_samples(rating_df, train_df, train_positive_movies)\n",
    "\n",
    "# # 为验证集和测试集生成负样本\n",
    "val_samples = generate_negative_samples(rating_df, val_df, val_positive_movies)\n",
    "test_samples = generate_negative_samples(rating_df, test_df,  test_positive_movies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_samples.count(), val_samples.count(), test_samples.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples.write.mode('overwrite').parquet('./train_df.parquet')\n",
    "val_samples.write.mode('overwrite').parquet('./val_df.parquet')\n",
    "test_samples.write.mode('overwrite').parquet('./test_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.select('user_id').distinct().count(),\\\n",
    "      train_df.select('movie_id').distinct().count(),\\\n",
    "      val_df.select('user_id').distinct().count(),\\\n",
    "      val_df.select('movie_id').distinct().count(),\\\n",
    "       val_df.count(), user_df.count(), movie_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_samples.count(), val_samples.count(), test_samples.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples.show(5)\n",
    "val_samples.show(5)\n",
    "test_samples.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证集电影更少是因为选择timestamp后每个用户5-10次的电影作为验证集，覆盖比较有限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"TowTowerModel\").config(\n",
    "    \"spark.driver.memory\", \"8g\").getOrCreate()\n",
    "\n",
    "# 加载数据集\n",
    "train_df = spark.read.parquet(\"./train_df.parquet\")\n",
    "val_df = spark.read.parquet(\"./val_df.parquet\")\n",
    "user_df = spark.read.parquet(\"./user_df.parquet\")\n",
    "movie_df = spark.read.parquet(\"./movie_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def save_large_data_to_multiple_files(rdd, batch_size, output_dir, base_filename):\n",
    "    rdd = rdd.zipWithIndex().cache()\n",
    "    num_rows = rdd.count()\n",
    "    num_batches = (num_rows + batch_size - 1) // batch_size\n",
    "\n",
    "    # 创建保存文件的目录\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # 分批保存数据\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min(start + batch_size, num_rows)\n",
    "\n",
    "        data_batch = rdd.filter(lambda x: start <= x[1] < end).map(lambda x: x[0]).collect()\n",
    "\n",
    "        user_features_batch = []\n",
    "        movie_features_batch = []\n",
    "        labels_batch = []\n",
    "\n",
    "        for row in data_batch:\n",
    "            user_id = torch.tensor(row['user_id']).unsqueeze(0).unsqueeze(1)\n",
    "            gender = torch.tensor(row['gender']).unsqueeze(0).unsqueeze(1)\n",
    "            age_vector = torch.tensor(row['ageVector']).unsqueeze(0)\n",
    "            occupation_vector = torch.tensor(row['occupationVector']).unsqueeze(0)\n",
    "            user_avg_rating_vector = torch.tensor(row['user_avg_ratingVector']).unsqueeze(0)\n",
    "            movie_id = torch.tensor(row['movie_id']).unsqueeze(0).unsqueeze(1)\n",
    "            genres_vector = torch.tensor(row['genresVector']).unsqueeze(0)\n",
    "            movie_avg_rating_vector = torch.tensor(row['movie_avg_ratingVector']).unsqueeze(0)\n",
    "            label = torch.tensor(row['label']).unsqueeze(0)\n",
    "\n",
    "            user_features = torch.cat([user_id, gender, age_vector, occupation_vector, user_avg_rating_vector], dim=1)\n",
    "            movie_features = torch.cat([movie_id, genres_vector, movie_avg_rating_vector], dim=1)\n",
    "\n",
    "            user_features_batch.append(user_features)\n",
    "            movie_features_batch.append(movie_features)\n",
    "            labels_batch.append(label)\n",
    "\n",
    "        user_features_batch = torch.stack(user_features_batch)\n",
    "        movie_features_batch = torch.stack(movie_features_batch)\n",
    "        labels_batch = torch.stack(labels_batch)\n",
    "\n",
    "        # 保存到 .pt 文件\n",
    "        filename = f\"{base_filename}_batch_{i+1}.pt\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        torch.save((user_features_batch, movie_features_batch, labels_batch), filepath)\n",
    "        print(f\"Saved batch {i+1}/{num_batches} to {filepath}\")\n",
    "\n",
    "# 将 DataFrame 转换为 RDD\n",
    "train_rdd = train_df.rdd\n",
    "val_rdd = val_df.rdd\n",
    "\n",
    "# 参数设置\n",
    "output_dir = './new_two_tower_data'\n",
    "batch_size = 100000\n",
    "base_filename = 'train_data'\n",
    "\n",
    "# 分批保存 train_rdd\n",
    "save_large_data_to_multiple_files(train_rdd, batch_size, output_dir, base_filename)\n",
    "base_filename = 'val_data'\n",
    "save_large_data_to_multiple_files(val_rdd, batch_size, output_dir, base_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
