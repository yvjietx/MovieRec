{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "print(pyspark.__version__)\n",
    "spark = SparkSession.builder.appName(\"windowFunctionExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Alice\", \"Sales\", 2000),\n",
    "    (\"Bob\", \"Sales\", 1500),\n",
    "    (\"Alice\", \"Engineering\", 2500),\n",
    "    (\"David\", \"Engineering\", 3000),\n",
    "    (\"Bob\", \"Engineering\", 2000)\n",
    "]\n",
    "columns = [\"name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, schema=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df1 = df.groupBy('name').agg(F.count('*').alias('count'))\n",
    "df1 = df.groupBy('name').count().withColumnRenamed('count', 'count')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 检查 GPU 是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"当前使用的设备是: {device}\")\n",
    "\n",
    "# 查询 GPU 设备信息\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 名称: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 数量: {torch.cuda.device_count()}\")\n",
    "\n",
    "# 定义一个简单的神经网络模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 实例化模型并将其移动到 GPU\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# 创建一些随机数据并通过模型执行前向传播\n",
    "# 假设我们有一个大小为 (batch_size, input_size) 的输入\n",
    "batch_size = 5\n",
    "input_size = 10\n",
    "inputs = torch.randn(batch_size, input_size).to(device)\n",
    "outputs = model(inputs)\n",
    "\n",
    "print(f\"模型的输出是: {outputs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = 4\n",
    "b = [0, 1, 0, 0]\n",
    "a = torch.tensor(a, dtype=torch.long).unsqueeze(0).unsqueeze(1).unsqueeze(0)\n",
    "a = a.squeeze(1)\n",
    "print(a.shape)\n",
    "a.unsqueeze(-1)\n",
    "print(a.size())\n",
    "b = torch.tensor(b, dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "print(b.size())\n",
    "res = torch.cat([a, b], dim=2)\n",
    "print(res)\n",
    "print(res[:,:, 0:2])\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FeatureEmbedder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(FeatureEmbedder, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "a_embedder = FeatureEmbedder(6, 4)\n",
    "# 创建一个张量 a，包含 6 个不同的取值\n",
    "a_values = res\n",
    "print(a_values[:,:,1:].shape)\n",
    "\n",
    "# 获取 a 的嵌入向量表示\n",
    "a_embeddings = a_embedder(a_values[:,:, 0].long())\n",
    "\n",
    "# 打印嵌入向量\n",
    "print(\"a 的嵌入向量表示:\")\n",
    "print(a_embeddings)\n",
    "print(a_embeddings.shape)\n",
    "\n",
    "z = torch.cat([a_embeddings, a_values[:,:, 1:]], dim=2)\n",
    "print(z)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn\n",
    "  \n",
    "# 创建一个一维张量  \n",
    "x = torch.tensor([[3.0], [1.0], [2.0]])  \n",
    "y = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0]])\n",
    "print(y.shape)\n",
    "for i in range(y.size(0)):\n",
    "    print(y[i, 0].item())\n",
    "\n",
    "# 升序排序的索引  \n",
    "ascending_indices = torch.argsort(x, dim=0)  \n",
    "print(ascending_indices)  \n",
    "  \n",
    "# 降序排序的索引  \n",
    "descending_indices = torch.argsort(x, dim=0, descending=True)  \n",
    "print(descending_indices)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "  \n",
    "# 创建三个形状为(2, 3)的张量  \n",
    "t1 = torch.tensor([[1, 2, 3]])  \n",
    "t2 = torch.tensor([[7, 8, 9]])  \n",
    "t3 = torch.tensor([[13, 14, 15]])  \n",
    "t4 = torch.tensor([[17, 18, 19]])\n",
    "t12 = torch.cat([t1, t2], dim=1)\n",
    "t34 = torch.cat([t3, t4], dim=1)\n",
    "print(t12.shape)\n",
    "t = []\n",
    "t.append(t12)\n",
    "t.append(t34)\n",
    "# 沿着第0维堆叠这三个张量  \n",
    "print(t)\n",
    "stacked_tensors = torch.stack(t)  \n",
    "  \n",
    "print(stacked_tensors.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 8\n",
    "ans = []\n",
    "\n",
    "# 生成随机整数张量作为输入，假设输入数据的每个元素都是从0到9的整数\n",
    "x = [1, 2, 3]\n",
    "y = [2, 3, 4]\n",
    "ans.append(x)\n",
    "ans.append(y)\n",
    "\n",
    "print(torch.tensor(ans))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([1,2]).unsqueeze(0)\n",
    "b = torch.tensor([3,4]).unsqueeze(0)\n",
    "c = torch.cat([a, b], dim=0)\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# 初始化Spark会话\n",
    "spark = SparkSession.builder.appName(\"Rating Vectors Example\").getOrCreate()\n",
    "\n",
    "# 定义数据结构\n",
    "schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"rating\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# 示例数据\n",
    "data = [\n",
    "    {'userId': 123, 'movieId': 45, 'rating': 4},\n",
    "    {'userId': 123, 'movieId': 56, 'rating': 5},\n",
    "    {'userId': 789, 'movieId': 45, 'rating': 3},\n",
    "    {'userId': 123, 'movieId': 45, 'rating': 3},\n",
    "    {'userId': 789, 'movieId': 56, 'rating': 4}\n",
    "]\n",
    "\n",
    "# 将Python列表转换为DataFrame\n",
    "rating_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# 显示DataFrame的内容\n",
    "rating_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "\n",
    "\n",
    "# 创建一个用户评分向量的UDF\n",
    "def create_rating_vector(ratings):\n",
    "    # 初始化一个长度为5的向量，表示评分1到5\n",
    "    rating_vector = [0] * 5\n",
    "    # 遍历评分，对相应的位置进行计数\n",
    "    for rating in ratings:\n",
    "        rating_vector[rating - 1] += 1\n",
    "    # 返回向量\n",
    "    return Vectors.dense(rating_vector)\n",
    "\n",
    "# 注册UDF\n",
    "create_rating_vector_udf = F.udf(create_rating_vector, VectorUDT())\n",
    "\n",
    "# 聚合每个用户的评分\n",
    "user_rating_vector_df = rating_df.groupBy(\"userId\").agg(\n",
    "    create_rating_vector_udf(F.collect_list(\"rating\")).alias(\"user_rating_vector\"),\n",
    "    F.count(\"rating\").alias(\"ratings_count\")\n",
    ")\n",
    "\n",
    "# 计算最终的用户评分向量\n",
    "def normalize_vector(vector, count):\n",
    "    return Vectors.dense([x / count for x in vector])\n",
    "\n",
    "normalize_vector_udf = F.udf(normalize_vector, VectorUDT())\n",
    "\n",
    "user_rating_vector_df = user_rating_vector_df.withColumn(\n",
    "    \"normalized_user_rating_vector\",\n",
    "    normalize_vector_udf(F.col(\"user_rating_vector\"), F.col(\"ratings_count\"))\n",
    ")\n",
    "\n",
    "# 同理，计算电影评分向量\n",
    "movie_rating_vector_df = rating_df.groupBy(\"movieId\").agg(\n",
    "    create_rating_vector_udf(F.collect_list(\"rating\")).alias(\"movie_rating_vector\"),\n",
    "    F.count(\"rating\").alias(\"ratings_count\")\n",
    ")\n",
    "\n",
    "movie_rating_vector_df = movie_rating_vector_df.withColumn(\n",
    "    \"normalized_movie_rating_vector\",\n",
    "    normalize_vector_udf(F.col(\"movie_rating_vector\"), F.col(\"ratings_count\"))\n",
    ")\n",
    "\n",
    "# 显示结果\n",
    "user_rating_vector_df.show(truncate=False)\n",
    "movie_rating_vector_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "a = torch.tensor([1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.window import Window\n",
    "\n",
    "    # 初始化Spark会话\n",
    "    spark = SparkSession.builder.appName(\"RandomAndWeightSort\").getOrCreate()\n",
    "\n",
    "    # 假设movie_click_freq DataFrame已经存在，并且包含movie_id, click_count, sampling_weight列\n",
    "    # 示例数据结构\n",
    "    movie_click_freq = spark.createDataFrame([\n",
    "        (1, 1, 0.5),\n",
    "        (1, 2, 0.6),\n",
    "        (2, 1, 0.7),\n",
    "        (1, 3, 0.8),\n",
    "        (1, 4, 0.3),\n",
    "        (1, 5, 0.6),\n",
    "        (1, 6, 0.7),\n",
    "        (2, 2, 0.9),\n",
    "        (2, 3, 0.8),\n",
    "        (3, 3, 0.8),\n",
    "        (4, 3, 0.4)\n",
    "    ], [\"user\", \"movie\", \"sampling_weight\"])\n",
    "\n",
    "    # 定义窗口规范\n",
    "    window_spec = Window.partitionBy(\"user\").orderBy(F.rand().multiply(0.1).cast(\"int\"), F.col(\"sampling_weight\").desc())\n",
    "\n",
    "\n",
    "    # 在窗口规范下，为每个用户的电影分配行号\n",
    "    user_negative_movies = movie_click_freq.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "\n",
    "    # 显示结果，这里可以替换为你的具体逻辑，比如选择行号小于某个阈值的数据作为负样本\n",
    "    user_negative_movies.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停止Spark会话\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
