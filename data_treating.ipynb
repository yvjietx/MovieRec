{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、标签，rating大于3为正样本，其余的为负样本 \\\n",
    "2、将每个用户timestamp最大的前5个样本作为测试集，其余的样本作为训练集 \\\n",
    "  注意：测试集有接近500的用户都是负样本，再评估排序模型时将这部分用户去掉，或者不去除，看效果如何。\\\n",
    "3、生成负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/31 14:30:24 WARN Utils: Your hostname, yvjie-Lenovo-Legion-Y7000-2019-1050 resolves to a loopback address: 127.0.1.1; using 172.24.70.50 instead (on interface wlp0s20f3)\n",
      "24/08/31 14:30:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/31 14:30:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession  \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window  \n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 初始化SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MovieLens Recommendation System\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path_to_users_data = \"./ml-1m/users.dat\"\n",
    "path_to_movies_data = \"./ml-1m/movies.dat\"\n",
    "path_to_ratings_data = \"./ml-1m/ratings.dat\"\n",
    "\n",
    "users_column_names = [\"user_id\", \"gender\", \"age\", \"occupation\", \"zip_code\"]\n",
    "movies_column_names = [\"movie_id\", \"title\", \"genres\"]\n",
    "ratings_column_names = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
    "\n",
    "user_df = spark.read.csv(path_to_users_data,\n",
    "                          sep='::',\n",
    "                          header=False,\n",
    "                          inferSchema=True)\n",
    "movie_df = spark.read.csv(path_to_movies_data,\n",
    "                           sep='::',\n",
    "                           header=False,\n",
    "                           inferSchema=True)\n",
    "rating_df = spark.read.csv(path_to_ratings_data,\n",
    "                            sep='::',\n",
    "                            header=False,\n",
    "                            inferSchema=True)\n",
    "user_df = user_df.toDF(*users_column_names).drop(\"zip_code\")\n",
    "movie_df = movie_df.toDF(*movies_column_names)\n",
    "rating_df = rating_df.toDF(*ratings_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ratings_pandas = rating_df.select('rating').toPandas()\n",
    "# 使用matplotlib绘制直方图\n",
    "plt.hist(ratings_pandas['rating'], bins=20, alpha=0.75)\n",
    "plt.title('Rating Distribution')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = rating_df.join(movie_df, 'movie_id', 'left')\n",
    "\n",
    "# 使用 StringIndexer 对 movie_id 进行重新编码\n",
    "indexer = StringIndexer(inputCol=\"movie_id\", outputCol=\"movie_id_encoded\")\n",
    "indexed_model = indexer.fit(rating_df)\n",
    "rating_df_encoded = indexed_model.transform(rating_df)\n",
    "rating_df = rating_df_encoded.drop(\n",
    "    *[\"movie_id\", 'title']).withColumnRenamed(\"movie_id_encoded\", \"movie_id\")\n",
    "rating_df = rating_df.withColumn(\"movie_id\", F.col(\"movie_id\").cast(\"integer\"))\n",
    "\n",
    "# 获得重新编码的电影数据集\n",
    "movie_df = rating_df.select(['movie_id', 'genres']).distinct()\n",
    "\n",
    "# 打标签\n",
    "rating_df = rating_df.withColumn(\"label\", F.when(F.col(\"rating\") > 2, 1).otherwise(\n",
    "    0)).select(['user_id', 'movie_id', 'label', 'rating', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建窗口规范，按user_id分区，按timestamp降序排序\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(F.desc(\"timestamp\"))\n",
    "# 添加行号\n",
    "data_spark_df = rating_df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "test_df = data_spark_df.filter(F.col(\"row_num\") <= 5).drop(*['timestamp', 'row_num'])\n",
    "val_df = data_spark_df.filter((F.col(\"row_num\") > 5) & (F.col(\"row_num\") <= 10)).drop(*['timestamp', 'row_num'])\n",
    "train_df = data_spark_df.filter(F.col(\"row_num\") > 10).drop(*['timestamp', 'row_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "users_with_label_1 = val_df.filter(F.col(\"label\") == 0).groupBy(\"user_id\").count()\n",
    "users_without_label_1 = users_with_label_1.filter(F.col(\"count\") == 5).select(\"user_id\").collect()\n",
    "users_without_label_1_list = [row['user_id'] for row in users_without_label_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./data/users_without_label_1_list.json', 'w') as file:\n",
    "    json.dump(users_without_label_1_list, file)\n",
    "\n",
    "# # 从JSON文件读取列表\n",
    "# with open('./data/users_without_label_1_list.json', 'r') as file:\n",
    "#     my_list = json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "\n",
    "# # 创建一个用户评分向量的UDF\n",
    "# def create_rating_vector(df):\n",
    "#     # 初始化一个长度为5的向量，表示评分1到5\n",
    "#     rating_vector = [0] * 5\n",
    "#     # 遍历评分，对相应的位置进行计数\n",
    "#     for rating in df:\n",
    "#         rating_vector[rating - 1] += 1\n",
    "#     # 返回向量\n",
    "#     return Vectors.dense(rating_vector)\n",
    "\n",
    "# 创建一个 UDF (User Defined Function) 来进行评分的分段\n",
    "def rating_segment(avg_rating):\n",
    "    if 1 <= avg_rating < 1.5:\n",
    "        return 1\n",
    "    elif 1.5 <= avg_rating < 2:\n",
    "        return 2\n",
    "    elif 2 <= avg_rating < 2.5:\n",
    "        return 3\n",
    "    elif 2.5 <= avg_rating < 3:\n",
    "        return 4\n",
    "    elif 3 <= avg_rating < 3.5:\n",
    "        return 5\n",
    "    elif 3.5 <= avg_rating < 4:\n",
    "        return 6\n",
    "    elif 4 <= avg_rating < 4.5:\n",
    "        return 7\n",
    "    elif 4.5 <= avg_rating <= 5:\n",
    "        return 8\n",
    "    else:\n",
    "        return None  # 处理无效的评分\n",
    "\n",
    "# 注册UDF\n",
    "# create_rating_vector_udf = F.udf(create_rating_vector, VectorUDT())\n",
    "segmented_rating_udf = F.udf(rating_segment)\n",
    "\n",
    "window_user = Window.partitionBy(\"user_id\")\n",
    "user_avg_rating_df = rating_df.withColumn('user_avg_rating', F.avg('rating').over(window_user))\n",
    "user_avg_rating_df = user_avg_rating_df.withColumn('user_avg_rating', segmented_rating_udf(F.col('user_avg_rating')))\n",
    "\n",
    "window_movie = Window.partitionBy(\"movie_id\")\n",
    "movie_avg_rating_df = rating_df.withColumn('movie_avg_rating', F.avg('rating').over(window_movie))\n",
    "movie_avg_rating_df = movie_avg_rating_df.withColumn('movie_avg_rating', segmented_rating_udf(F.col('movie_avg_rating')))\n",
    "\n",
    "# # 聚合每个用户的评分\n",
    "# user_rating_vector_df = train_df.groupBy(\"user_id\").agg(\n",
    "#     create_rating_vector_udf(F.collect_list(\"rating\")).alias(\"user_rating_vector\"),\n",
    "#     F.count(\"rating\").alias(\"ratings_count\")\n",
    "# )\n",
    "\n",
    "# # 计算最终的用户评分向量\n",
    "# def normalize_vector(vector, count):\n",
    "#     return Vectors.dense([x / count for x in vector])\n",
    "\n",
    "# normalize_vector_udf = F.udf(normalize_vector, VectorUDT())\n",
    "\n",
    "# user_rating_vector_df = user_rating_vector_df.withColumn(\n",
    "#     \"normalized_user_rating_vector\",\n",
    "#     normalize_vector_udf(F.col(\"user_rating_vector\"), F.col(\"ratings_count\"))\n",
    "# )\n",
    "\n",
    "# # 同理，计算电影评分向量\n",
    "# movie_rating_vector_df = train_df.groupBy(\"movie_id\").agg(\n",
    "#     create_rating_vector_udf(F.collect_list(\"rating\")).alias(\"movie_rating_vector\"),\n",
    "#     F.count(\"rating\").alias(\"ratings_count\")\n",
    "# )\n",
    "\n",
    "# movie_rating_vector_df = movie_rating_vector_df.withColumn(\n",
    "#     \"normalized_movie_rating_vector\",\n",
    "#     normalize_vector_udf(F.col(\"movie_rating_vector\"), F.col(\"ratings_count\"))\n",
    "# )\n",
    "\n",
    "user_avg_rating_df = user_avg_rating_df.select(['user_id', 'user_avg_rating']).distinct()\n",
    "movie_avg_rating_df = movie_avg_rating_df.select(['movie_id', 'movie_avg_rating']).distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_df = user_df.join(user_rating_vector_df, 'user_id', 'left')\n",
    "# movie_df = movie_df.join(movie_rating_vector_df, 'movie_id', 'left')\n",
    "user_df = user_df.join(user_avg_rating_df, 'user_id', 'left')\n",
    "movie_df = movie_df.join(movie_avg_rating_df, 'movie_id', 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定义UDF来创建一个长度为5的零向量\n",
    "# def create_zero_vector():\n",
    "#     return Vectors.dense([0, 0, 0, 0, 0])\n",
    "\n",
    "# # 注册UDF\n",
    "# zero_vector_udf = F.udf(create_zero_vector, VectorUDT())\n",
    "\n",
    "# # 使用UDF来填充normalized_movie_rating_vector列中为null的行\n",
    "# movie_df = movie_df.withColumn(\n",
    "#     \"normalized_movie_rating_vector\",\n",
    "#     F.when(F.col(\"normalized_movie_rating_vector\").isNull(), zero_vector_udf()).otherwise(\n",
    "#         F.col(\"normalized_movie_rating_vector\")\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 225:=========>                                               (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/31 13:05:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 229:=========>                                               (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/31 13:05:13 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.write.mode('overwrite').parquet('./data/train_df.parquet')\n",
    "val_df.write.mode('overwrite').parquet('./data/val_df.parquet')\n",
    "test_df.write.mode('overwrite').parquet('./data/test_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def OneHotEncoder(df, col_name):\n",
    "    unique_col = df.select(col_name).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    unique_col.sort()\n",
    "    for item in unique_col:\n",
    "        df = df.withColumn(str(item), F.when(F.col(col_name)==item, 1).otherwise(0))\n",
    "    # 使用VectorAssembler将多个二进制列合并成一个向量列\n",
    "    assembler = VectorAssembler(inputCols=[str(item) for item in unique_col],\n",
    "                                outputCol=col_name+'Vector')\n",
    "    df = assembler.transform(df)\n",
    "    # 删除中间创建的one-hot编码列\n",
    "    for item in unique_col:\n",
    "        df = df.drop(str(item))\n",
    "    df = df.drop(col_name)\n",
    "    return df\n",
    "\n",
    "def MultiHotEncoder(df, col_name):\n",
    "    # 对genres进行多热编码\n",
    "    # 拆分genres列\n",
    "    col_split = df.withColumn(col_name, F.explode(F.split(F.col(col_name), \"\\\\|\")))\n",
    "    # 获取所有genre的列表\n",
    "    unique_col = col_split.select(col_name).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    # 对每个genre进行multi-hot编码\n",
    "    for item in unique_col:\n",
    "        if isinstance(item, str):\n",
    "            df = df.withColumn(item, F.when(F.col(col_name).contains(item), 1).otherwise(0))\n",
    "    # 使用VectorAssembler将多个二进制列合并成一个向量列\n",
    "    assembler = VectorAssembler(inputCols=[item for item in unique_col if isinstance(item, str)],\n",
    "                                outputCol=col_name+'Vector')\n",
    "    df = assembler.transform(df)\n",
    "    # 删除中间创建的one-hot编码列\n",
    "    for item in unique_col:\n",
    "        if isinstance(item, str):\n",
    "            df = df.drop(item)\n",
    "    df = df.drop(col_name)\n",
    "    return df\n",
    "    \n",
    "def user_data_processing(df):\n",
    "    # 将gender改为二值变量\n",
    "    df = df.withColumn(\"gender\",\n",
    "                                F.when(df[\"gender\"] == \"F\", 0).otherwise(1))\n",
    "    # 对age进行独热编码\n",
    "    df = OneHotEncoder(df, 'age')\n",
    "    # 对occupation进行独热编码\n",
    "    df = OneHotEncoder(df, 'occupation')\n",
    "    df = OneHotEncoder(df, 'user_avg_rating')\n",
    "\n",
    "    return df\n",
    "\n",
    "def movie_data_processing(df):\n",
    "    # 对genres进行独热编码\n",
    "    df = MultiHotEncoder(df, 'genres')\n",
    "    df = OneHotEncoder(df, 'movie_avg_rating')\n",
    "    return df\n",
    "\n",
    "    \n",
    "\n",
    "# 假设data_df是合并后的DataFrame\n",
    "user_df = user_data_processing(user_df)\n",
    "movie_df = movie_data_processing(movie_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df.write.mode('overwrite').parquet('./data/user_df.parquet')\n",
    "movie_df.write.mode('overwrite').parquet('./data/movie_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df = spark.read.parquet('./data/user_df.parquet')\n",
    "movie_df = spark.read.parquet('./data/movie_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 定义一个函数来将向量转换为字符串列表\n",
    "def vector_to_list(vector):\n",
    "    return vector.toArray().tolist()\n",
    "\n",
    "# 将DataFrame转换为RDD，并将每行数据转换为(user_id, [其他列值])的元组\n",
    "user_rdd = user_df.rdd.map(lambda row: (\n",
    "    row['user_id'], [row['user_id'], row['gender']] \n",
    "    + vector_to_list(row['ageVector']) \n",
    "    + vector_to_list(row['occupationVector']) \n",
    "    + vector_to_list(row['user_avg_ratingVector'])\n",
    "    ))\n",
    "\n",
    "movie_rdd = movie_df.rdd.map(lambda row: (\n",
    "    row['movie_id'], [row['movie_id']]\n",
    "    + vector_to_list(row['genresVector'])\n",
    "    + vector_to_list(row['movie_avg_ratingVector'])\n",
    "    ))\n",
    "\n",
    "# 收集RDD中的所有元素，并转换为字典\n",
    "user_dict = user_rdd.collectAsMap()\n",
    "movie_dict = movie_rdd.collectAsMap()\n",
    "\n",
    "# 保存字典到文件\n",
    "with open('./data/user_FeatureVector_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(user_dict, f)\n",
    "\n",
    "with open('./data/movie_FeatureVector_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(movie_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/31 02:29:26 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"NegativeSampling\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read.parquet('./data/train_df.parquet')\n",
    "val_df = spark.read.parquet('./data/val_df.parquet')\n",
    "df = train_df.union(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_samples(train_df, sampling_ratio=2):\n",
    "    all_user_df = train_df.select('user_id').distinct()\n",
    "    all_movie_df = train_df.select('movie_id').distinct()\n",
    "    all_user_movie_df  = all_user_df.select('user_id').crossJoin(all_movie_df)\n",
    "\n",
    "    # 计算每个电影的点击频率\n",
    "    movie_click_freq = train_df.groupBy(\"movie_id\").count().withColumnRenamed(\"count\", \"click_count\").select('movie_id', 'click_count')\n",
    "    # 计算每个电影的权重（点击频率的0.75次幂）\n",
    "    movie_click_freq = movie_click_freq.withColumn(\"sampling_weight\", F.pow(F.col(\"click_count\"), 0.75)).drop(\"click_count\")\n",
    "\n",
    "    # 计算每个用户的正负样本数量\n",
    "    user_positive_movie = train_df.where(F.col('label')==1).select(\"user_id\", \"movie_id\")\n",
    "    user_positive_count = user_positive_movie.groupBy(\"user_id\").count().withColumnRenamed(\"count\", \"positive_count\")\n",
    "    user_negative_movie = train_df.where(F.col('label')==0).select(\"user_id\", \"movie_id\")\n",
    "    user_negative_count = user_negative_movie.groupBy(\"user_id\").count().withColumnRenamed(\"count\", \"negative_count\")\n",
    "    # 合并正样本和负样本的数量\n",
    "    user_counts = user_positive_count.join(user_negative_count, on=\"user_id\", how=\"outer\").fillna(0)\n",
    "\n",
    "    # 计算两倍正样本数减去负样本数，即每个用户应该生成的负样本数\n",
    "    user_counts = user_counts.withColumn(\"user_negative_count\", sampling_ratio * F.col(\"positive_count\") - F.col(\"negative_count\")).drop(*[\"positive_count\", \"negative_count\"])\n",
    "\n",
    "    # 所有用户-电影组合中，没有点击过的电影\n",
    "    user_negative_movies = all_user_movie_df.join(user_positive_movie, on=[\"user_id\", \"movie_id\"], how=\"left_anti\")\n",
    "\n",
    "    # 筛选用户负样本\n",
    "    # 合并电影电影点击频率和权重以及用户应该生成负样本数\n",
    "    user_negative_movies = user_negative_movies.join(user_counts, 'user_id', 'left').join(movie_click_freq, 'movie_id', 'left')\n",
    "\n",
    "    # 窗口是按user_id进行分区，随机排序后，按weight逆序排列\n",
    "    window_spec = Window.partitionBy(\"user_id\").orderBy(F.rand(42), F.col(\"sampling_weight\").desc())\n",
    "    user_negative_movies = user_negative_movies.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "    \n",
    "    # 生成负样本\n",
    "    user_negative_movies = user_negative_movies.filter(F.col(\"row_num\") <=  F.col(\"user_negative_count\"))  \n",
    "\n",
    "    # 给生成的负样本打标签并合并到训练集中\n",
    "    user_negative_movies = user_negative_movies.withColumn(\"label\", F.lit(0)).withColumn('rating', F.lit(1)).select('user_id', 'movie_id', 'label', 'rating')\n",
    "    train_df = train_df.union(user_negative_movies)\n",
    "\n",
    "    return train_df\n",
    "\n",
    "train_df = generate_negative_samples(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_samples(train_df, all_user_movie_df, user_positive_count, user_positive_movie, sampling_ratio=2):\n",
    "\n",
    "    # 计算每个电影的点击频率\n",
    "    movie_click_freq = train_df.groupBy(\"movie_id\").count().withColumnRenamed(\"count\", \"click_count\").select('movie_id', 'click_count')\n",
    "    # 计算每个电影的权重（点击频率的0.75次幂）\n",
    "    movie_click_freq = movie_click_freq.withColumn(\"sampling_weight\", F.pow(F.col(\"click_count\"), 0.75)).drop(\"click_count\")\n",
    "\n",
    "    # 计算每个用户的正负样本数量\n",
    "    user_negative_movie = train_df.where(F.col('label')==0).select(\"user_id\", \"movie_id\")\n",
    "    user_negative_count = user_negative_movie.groupBy(\"user_id\").count().withColumnRenamed(\"count\", \"negative_count\")\n",
    "    # 合并正样本和负样本的数量\n",
    "    user_counts = user_positive_count.join(user_negative_count, on=\"user_id\", how=\"outer\").fillna(0)\n",
    "\n",
    "    # 计算两倍正样本数减去负样本数，即每个用户应该生成的负样本数\n",
    "    user_counts = user_counts.withColumn(\"user_negative_count\", sampling_ratio * F.col(\"positive_count\") - F.col(\"negative_count\")).drop(*[\"positive_count\", \"negative_count\"])\n",
    "\n",
    "    # 所有用户-电影组合中，没有点击过的电影\n",
    "    user_negative_movies = all_user_movie_df.join(user_positive_movie, on=[\"user_id\", \"movie_id\"], how=\"left_anti\")\n",
    "\n",
    "    # 筛选用户负样本\n",
    "    # 合并电影电影点击频率和权重以及用户应该生成负样本数\n",
    "    user_negative_movies = user_negative_movies.join(user_counts, 'user_id', 'left').join(movie_click_freq, 'movie_id', 'left')\n",
    "\n",
    "    # 窗口是按user_id进行分区，随机排序后，按weight逆序排列\n",
    "    window_spec = Window.partitionBy(\"user_id\").orderBy(F.rand(42), F.col(\"sampling_weight\").desc())\n",
    "    user_negative_movies = user_negative_movies.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "    \n",
    "    # 生成负样本\n",
    "    user_negative_movies = user_negative_movies.filter(F.col(\"row_num\") <=  F.col(\"user_negative_count\"))  \n",
    "\n",
    "    # 给生成的负样本打标签并合并到训练集中\n",
    "    user_negative_movies = user_negative_movies.withColumn(\"label\", F.lit(0)).withColumn('rating', F.lit(1)).select('user_id', 'movie_id', 'label', 'rating')\n",
    "    train_df = train_df.union(user_negative_movies)\n",
    "\n",
    "    return train_df\n",
    "\n",
    "\n",
    "# 所有用户-电影组合\n",
    "all_user_df = df.select('user_id').distinct()\n",
    "all_movie_df = df.select('movie_id').distinct()\n",
    "all_user_movie_df  = all_user_df.select('user_id').crossJoin(all_movie_df)\n",
    "\n",
    "user_positive_movie = df.where(F.col('label')==1).select(\"user_id\", \"movie_id\")\n",
    "user_positive_count = val_df.where(F.col('label')==1).select(\"user_id\", \"movie_id\").groupBy(\"user_id\").count().withColumnRenamed(\"count\", \"positive_count\")\n",
    "\n",
    "val_df = generate_negative_samples(val_df, all_user_movie_df, user_positive_count, user_positive_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.write.mode('overwrite').parquet('./data/train_set.parquet')\n",
    "val_df.write.mode('overwrite').parquet('./data/val_set.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
